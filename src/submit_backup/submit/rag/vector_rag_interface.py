"""
Drop-in –∑–∞–º–µ–Ω–∞ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –ø—Ä–æ–µ–∫—Ç GigaMemory
–ü—Ä–æ—Å—Ç–æ –∑–∞–º–µ–Ω–∏—Ç–µ RAGInterface –Ω–∞ VectorRAGInterface –≤ –≤–∞—à–µ–º –∫–æ–¥–µ!
"""
import torch
import numpy as np
from typing import List, Dict, Optional, Tuple, Any
import logging
from pathlib import Path
import json
from datetime import datetime

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
from ..embeddings.improved_vector_search import (
    ImprovedEmbeddingEngine,
    ImprovedVectorStore,
    EmbeddingConfig,
    SimilarityMetric
)

logger = logging.getLogger(__name__)


class VectorRAGInterface:
    """
    Drop-in –∑–∞–º–µ–Ω–∞ –¥–ª—è RAGInterface —Å –≤–µ–∫—Ç–æ—Ä–Ω—ã–º –ø–æ–∏—Å–∫–æ–º
    –ü–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–≤–º–µ—Å—Ç–∏–º–∞ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º API
    """
    
    def __init__(self, 
                 model_name: str = "cointegrated/rubert-tiny2",
                 use_gpu: bool = True,
                 enable_hybrid_search: bool = True):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        
        Args:
            model_name: –ú–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            use_gpu: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
            enable_hybrid_search: –í–∫–ª—é—á–∏—Ç—å –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫
        """
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        device = "cuda" if (use_gpu and torch.cuda.is_available()) else "cpu"
        
        # –°–æ–∑–¥–∞–µ–º –¥–≤–∏–∂–æ–∫ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.embedding_engine = ImprovedEmbeddingEngine(
            EmbeddingConfig(
                model_name=model_name,
                device=device,
                batch_size=32,
                use_cache=True,
                use_amp=device == "cuda"
            )
        )
        
        # –•—Ä–∞–Ω–∏–ª–∏—â–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–∏–∞–ª–æ–≥–∞
        self.stores = {}
        self.enable_hybrid = enable_hybrid_search
        
        # –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º RAGInterface
        self.dialogues = {}  # –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        
        logger.info(f"VectorRAGInterface –≥–æ—Ç–æ–≤: {model_name} –Ω–∞ {device}")
    
    def add_dialogue(self, dialogue_id: str, messages: List[Dict[str, str]]):
        """
        –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–∏–∞–ª–æ–≥–∞ (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º API)
        
        Args:
            dialogue_id: ID –¥–∏–∞–ª–æ–≥–∞
            messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π
        """
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        self.dialogues[dialogue_id] = messages
        
        # –°–æ–∑–¥–∞–µ–º —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –µ—Å–ª–∏ –Ω–µ—Ç
        if dialogue_id not in self.stores:
            self.stores[dialogue_id] = ImprovedVectorStore(
                metric=SimilarityMetric.COSINE,
                enable_analytics=True
            )
        
        # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
        self._index_messages(dialogue_id, messages)
    
    def get_relevant_context(self, 
                            query: str, 
                            dialogue_id: str,
                            top_k: int = 5) -> str:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º API)
        
        Args:
            query: –ó–∞–ø—Ä–æ—Å
            dialogue_id: ID –¥–∏–∞–ª–æ–≥–∞
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            –°—Ç—Ä–æ–∫–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        """
        if dialogue_id not in self.stores:
            return ""
        
        # –ö–æ–¥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å
        query_embedding = self.embedding_engine.encode(query)
        
        # –ò—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
        store = self.stores[dialogue_id]
        
        if self.enable_hybrid:
            results = store.hybrid_search(
                query_vector=query_embedding,
                query_text=query,
                k=top_k
            )
        else:
            results = store.search(
                query_vector=query_embedding,
                k=top_k,
                threshold=0.7
            )
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        if not results:
            return ""
        
        context_parts = ["–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞:"]
        
        for result in results:
            if result.text:
                context_parts.append(f"- {result.text}")
        
        return "\n".join(context_parts)
    
    def search(self, query: str, dialogue_id: str, **kwargs) -> List[Dict]:
        """
        –ü–æ–∏—Å–∫ (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º API)
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ñ–æ—Ä–º–∞—Ç–µ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ RAGInterface
        """
        if dialogue_id not in self.stores:
            return []
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
        query_embedding = self.embedding_engine.encode(query)
        store = self.stores[dialogue_id]
        
        results = store.search(
            query_vector=query_embedding,
            k=kwargs.get('top_k', 5),
            threshold=kwargs.get('threshold', 0.7)
        )
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ API
        return [
            {
                'id': r.doc_id,
                'score': r.score,
                'text': r.text,
                'metadata': r.metadata
            }
            for r in results
        ]
    
    def process_message(self, 
                       message: str,
                       dialogue_id: str,
                       role: str = "user") -> str:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–µ–π
        
        Args:
            message: –¢–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è
            dialogue_id: ID –¥–∏–∞–ª–æ–≥–∞
            role: –†–æ–ª—å (user/assistant)
            
        Returns:
            –û–±–æ–≥–∞—â–µ–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        """
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –¥–∏–∞–ª–æ–≥
        if dialogue_id not in self.dialogues:
            self.dialogues[dialogue_id] = []
        
        self.dialogues[dialogue_id].append({
            'role': role,
            'content': message
        })
        
        # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –µ—Å–ª–∏ —ç—Ç–æ —Å–æ–æ–±—â–µ–Ω–∏–µ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
        if role == "assistant":
            self._index_single_message(dialogue_id, message, role)
        
        # –î–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è - –∏—â–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        if role == "user":
            context = self.get_relevant_context(message, dialogue_id)
            if context:
                return f"{context}\n\n–í–æ–ø—Ä–æ—Å: {message}"
        
        return message
    
    def _index_messages(self, dialogue_id: str, messages: List[Dict[str, str]]):
        """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Å–ø–∏—Å–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π"""
        store = self.stores[dialogue_id]
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ 3-5 —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        chunk_size = 3
        chunks = []
        
        for i in range(0, len(messages), chunk_size):
            chunk = messages[i:i+chunk_size]
            chunk_text = "\n".join([
                f"{msg.get('role', 'unknown')}: {msg.get('content', '')}" 
                for msg in chunk
            ])
            chunks.append(chunk_text)
        
        # –ö–æ–¥–∏—Ä—É–µ–º –≤—Å–µ —á–∞–Ω–∫–∏
        if chunks:
            embeddings = self.embedding_engine.encode(chunks)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
            for i, (chunk_text, embedding) in enumerate(zip(chunks, embeddings)):
                store.add(
                    doc_id=f"chunk_{dialogue_id}_{i}",
                    vector=embedding,
                    text=chunk_text,
                    metadata={'chunk_index': i, 'dialogue_id': dialogue_id}
                )
    
    def _index_single_message(self, dialogue_id: str, message: str, role: str):
        """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –æ–¥–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è"""
        if dialogue_id not in self.stores:
            self.stores[dialogue_id] = ImprovedVectorStore(
                metric=SimilarityMetric.COSINE
            )
        
        store = self.stores[dialogue_id]
        embedding = self.embedding_engine.encode(message)
        
        store.add(
            doc_id=f"msg_{dialogue_id}_{datetime.now().timestamp()}",
            vector=embedding,
            text=f"{role}: {message}",
            metadata={'role': role, 'timestamp': datetime.now().isoformat()}
        )
    
    def save(self, path: str = "./rag_indices"):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤"""
        save_dir = Path(path)
        save_dir.mkdir(exist_ok=True, parents=True)
        
        for dialogue_id, store in self.stores.items():
            store.save(str(save_dir / f"{dialogue_id}.idx"))
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∏–∞–ª–æ–≥–∏
        with open(save_dir / "dialogues.json", 'w') as f:
            json.dump(self.dialogues, f)
        
        logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(self.stores)} –∏–Ω–¥–µ–∫—Å–æ–≤")
    
    def load(self, path: str = "./rag_indices"):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–æ–≤"""
        load_dir = Path(path)
        
        if not load_dir.exists():
            return
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∏–∞–ª–æ–≥–∏
        dialogues_file = load_dir / "dialogues.json"
        if dialogues_file.exists():
            with open(dialogues_file, 'r') as f:
                self.dialogues = json.load(f)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã
        for idx_file in load_dir.glob("*.idx"):
            dialogue_id = idx_file.stem
            store = ImprovedVectorStore(metric=SimilarityMetric.COSINE)
            store.load(str(idx_file))
            self.stores[dialogue_id] = store
        
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.stores)} –∏–Ω–¥–µ–∫—Å–æ–≤")


# ============== –ü—Ä–æ—Å—Ç–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ ==============

def integrate_into_gigamemory():
    """
    –ü—Ä–∏–º–µ—Ä –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª model_inference.py
    """
    code = '''
# –í —Ñ–∞–π–ª–µ src/submit/model_inference.py –∑–∞–º–µ–Ω–∏—Ç–µ:

# –ë–´–õ–û:
# from .rag.rag_engine import RAGInterface

# –°–¢–ê–õ–û:
from .rag.vector_rag_interface import VectorRAGInterface

class SubmitModelWithMemory(ModelWithMemory):
    def __init__(self, model_path: str) -> None:
        self.storage = MemoryStorage()
        self.model_inference = ModelInference(model_path)
        
        # –ë–´–õ–û:
        # self.rag_interface = RAGInterface()
        
        # –°–¢–ê–õ–û:
        self.rag_interface = VectorRAGInterface(
            model_name="cointegrated/rubert-tiny2",  # –ò–ª–∏ –¥—Ä—É–≥–∞—è –º–æ–¥–µ–ª—å
            use_gpu=True,  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
            enable_hybrid_search=True  # –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫
        )
    
    def update_dialogue(self, dialogue_id: str, messages: List[DialogueUtterance]) -> None:
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∏–∞–ª–æ–≥–∞ —Å –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–µ–π"""
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –Ω—É–∂–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        formatted_messages = [
            {"role": msg.role, "content": msg.content} 
            for msg in messages
        ]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ RAG (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è)
        self.rag_interface.add_dialogue(dialogue_id, formatted_messages)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ storage –∫–∞–∫ –æ–±—ã—á–Ω–æ
        self.storage.save_dialogue(dialogue_id, messages)
    
    def ask(self, 
            question: str, 
            dialogue_id: str, 
            model_params: Dict[str, Any]) -> str:
        """–û—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
        context = self.rag_interface.get_relevant_context(
            question, 
            dialogue_id,
            top_k=5  # –¢–æ–ø-5 —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
        )
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        if context:
            enhanced_prompt = f"{context}\\n\\n–í–æ–ø—Ä–æ—Å: {question}"
        else:
            enhanced_prompt = question
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        response = self.model_inference.generate(
            enhanced_prompt,
            **model_params
        )
        
        # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –¥–ª—è –±—É–¥—É—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        self.rag_interface.process_message(
            response,
            dialogue_id,
            role="assistant"
        )
        
        return response
'''
    return code


# ============== –¢–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç ==============

def test_integration():
    """
    –¢–µ—Å—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
    """
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ RAG –¥–ª—è GigaMemory")
    print("=" * 60)
    
    # 1. –°–æ–∑–¥–∞–µ–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
    rag = VectorRAGInterface(
        model_name="cointegrated/rubert-tiny2",
        use_gpu=torch.cuda.is_available(),
        enable_hybrid_search=True
    )
    
    # 2. –¢–µ—Å—Ç–æ–≤—ã–π –¥–∏–∞–ª–æ–≥
    dialogue_id = "test_dialogue"
    messages = [
        {"role": "user", "content": "–ü—Ä–∏–≤–µ—Ç! –ú–µ–Ω—è –∑–æ–≤—É—Ç –ê–ª–µ–∫—Å–µ–π, —è –∏–∑ –ú–æ—Å–∫–≤—ã."},
        {"role": "assistant", "content": "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ, –ê–ª–µ–∫—Å–µ–π! –†–∞–¥ –ø–æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è. –ö–∞–∫ –¥–µ–ª–∞ –≤ –ú–æ—Å–∫–≤–µ?"},
        {"role": "user", "content": "–û—Ç–ª–∏—á–Ω–æ! –Ø —Ä–∞–±–æ—Ç–∞—é –¥–∞—Ç–∞-—Å–∞–π–µ–Ω—Ç–∏—Å—Ç–æ–º –≤ –°–±–µ—Ä–µ."},
        {"role": "assistant", "content": "–ó–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–æ! Data Science - –æ—á–µ–Ω—å –∏–Ω—Ç–µ—Ä–µ—Å–Ω–∞—è –æ–±–ª–∞—Å—Ç—å."},
        {"role": "user", "content": "–£ –º–µ–Ω—è –µ—Å—Ç—å –∫–æ—à–∫–∞ –ú—É—Ä–∫–∞ –∏ —Å–æ–±–∞–∫–∞ –†–µ–∫—Å."},
        {"role": "assistant", "content": "–ó–¥–æ—Ä–æ–≤–æ! –ú—É—Ä–∫–∞ –∏ –†–µ–∫—Å - –ø—Ä–µ–∫—Ä–∞—Å–Ω—ã–µ –∏–º–µ–Ω–∞ –¥–ª—è –ø–∏—Ç–æ–º—Ü–µ–≤."},
        {"role": "user", "content": "–Ø —É–≤–ª–µ–∫–∞—é—Å—å –º–∞—à–∏–Ω–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º –∏ –Ω–µ–π—Ä–æ—Å–µ—Ç—è–º–∏."},
        {"role": "assistant", "content": "–û—Ç–ª–∏—á–Ω–æ–µ —É–≤–ª–µ—á–µ–Ω–∏–µ! ML –∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ —Å–µ–π—á–∞—Å –æ—á–µ–Ω—å –∞–∫—Ç—É–∞–ª—å–Ω—ã."},
    ]
    
    # 3. –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –¥–∏–∞–ª–æ–≥
    print("\nüìù –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–∏–∞–ª–æ–≥–∞...")
    rag.add_dialogue(dialogue_id, messages)
    print(f"‚úÖ –ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ {len(messages)} —Å–æ–æ–±—â–µ–Ω–∏–π")
    
    # 4. –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    test_queries = [
        "–ö–∞–∫ –º–µ–Ω—è –∑–æ–≤—É—Ç?",
        "–ì–¥–µ —è —Ä–∞–±–æ—Ç–∞—é?",
        "–ö–∞–∫–∏–µ —É –º–µ–Ω—è –ø–∏—Ç–æ–º—Ü—ã?",
        "–ß–µ–º —è —É–≤–ª–µ–∫–∞—é—Å—å?",
        "–û—Ç–∫—É–¥–∞ —è?"
    ]
    
    print("\nüîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞:")
    for query in test_queries:
        print(f"\n‚ùì –í–æ–ø—Ä–æ—Å: {query}")
        context = rag.get_relevant_context(query, dialogue_id, top_k=2)
        
        if context:
            print("üìö –ù–∞–π–¥–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç:")
            for line in context.split('\n')[1:3]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 2 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                if line.strip():
                    print(f"  {line[:100]}...")
        else:
            print("  ‚ùå –ö–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    # 5. –¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
    print("\nüí¨ –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞:")
    new_question = "–ö–∞–∫ –∑–æ–≤—É—Ç –º–æ—é –∫–æ—à–∫—É –∏ –≥–¥–µ —è —Ä–∞–±–æ—Ç–∞—é?"
    enhanced = rag.process_message(new_question, dialogue_id, role="user")
    print(f"–û–±–æ–≥–∞—â–µ–Ω–Ω—ã–π –≤–æ–ø—Ä–æ—Å:\n{enhanced[:300]}...")
    
    # 6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞
    print("\nüíæ –¢–µ—Å—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è/–∑–∞–≥—Ä—É–∑–∫–∏...")
    rag.save("./test_indices")
    
    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º
    rag2 = VectorRAGInterface()
    rag2.load("./test_indices")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∑–∞–≥—Ä—É–∑–∏–ª–æ—Å—å
    test_query = "–ø–∏—Ç–æ–º—Ü—ã"
    results = rag2.search(test_query, dialogue_id, top_k=1)
    
    if results:
        print(f"‚úÖ –ò–Ω–¥–µ–∫—Å—ã —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –ù–∞–π–¥–µ–Ω–æ: {results[0]['text'][:50]}...")
    else:
        print("‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω–¥–µ–∫—Å–æ–≤")
    
    # 7. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    if dialogue_id in rag.stores:
        store = rag.stores[dialogue_id]
        analytics = store.get_analytics()
        print(f"  –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å–µ: {analytics.get('total_documents', 0)}")
        print(f"  –í—Å–µ–≥–æ –ø–æ–∏—Å–∫–æ–≤: {analytics.get('total_searches', 0)}")
        print(f"  –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –ø–æ–∏—Å–∫–∞: {analytics.get('avg_search_time', 0)*1000:.2f} –º—Å")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    emb_stats = rag.embedding_engine.get_stats()
    print(f"  –ó–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤: {emb_stats.get('total_encoded', 0)}")
    print(f"  Cache hit rate: {emb_stats.get('cache_hit_rate', 0):.1%}")
    
    print("\n‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
    
    # –û—á–∏—Å—Ç–∫–∞
    import shutil
    if Path("./test_indices").exists():
        shutil.rmtree("./test_indices")


# ============== –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ —É—Å—Ç–∞–Ω–æ–≤–∫–µ ==============

def print_installation_guide():
    """
    –í—ã–≤–æ–¥–∏—Ç –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –ø–æ —É—Å—Ç–∞–Ω–æ–≤–∫–µ
    """
    guide = '''
üì¶ –ò–ù–°–¢–†–£–ö–¶–ò–Ø –ü–û –£–°–¢–ê–ù–û–í–ö–ï –í–ï–ö–¢–û–†–ù–û–ì–û –ü–û–ò–°–ö–ê –í GIGAMEMORY
=========================================================

1Ô∏è‚É£ –ö–û–ü–ò–†–û–í–ê–ù–ò–ï –§–ê–ô–õ–û–í:
   ```bash
   # –°–æ–∑–¥–∞–π—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–∞–ø–æ–∫
   mkdir -p src/submit/embeddings
   mkdir -p src/submit/rag
   
   # –°–∫–æ–ø–∏—Ä—É–π—Ç–µ —Ñ–∞–π–ª—ã:
   - improved_vector_search.py ‚Üí src/submit/embeddings/
   - vector_rag_integration.py ‚Üí src/submit/rag/
   - vector_rag_interface.py ‚Üí src/submit/rag/
   ```

2Ô∏è‚É£ –û–ë–ù–û–í–õ–ï–ù–ò–ï –ó–ê–í–ò–°–ò–ú–û–°–¢–ï–ô:
   –í—Å–µ –Ω—É–∂–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã:
   ‚úÖ torch
   ‚úÖ transformers
   ‚úÖ numpy

3Ô∏è‚É£ –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –í –ö–û–î:
   
   –í–∞—Ä–∏–∞–Ω—Ç A: –ü–æ–ª–Ω–∞—è –∑–∞–º–µ–Ω–∞ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
   -----------------------------------------
   –í —Ñ–∞–π–ª–µ src/submit/model_inference.py:
   
   ```python
   # –ó–∞–º–µ–Ω–∏—Ç–µ –∏–º–ø–æ—Ä—Ç
   from .rag.vector_rag_interface import VectorRAGInterface
   
   # –í __init__ –∑–∞–º–µ–Ω–∏—Ç–µ
   self.rag_interface = VectorRAGInterface(
       model_name="cointegrated/rubert-tiny2",
       use_gpu=True,
       enable_hybrid_search=True
   )
   ```
   
   –í–∞—Ä–∏–∞–Ω—Ç B: –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
   --------------------------------------
   ```python
   def __init__(self, model_path: str, use_vector_rag: bool = True):
       if use_vector_rag:
           from .rag.vector_rag_interface import VectorRAGInterface
           self.rag_interface = VectorRAGInterface()
       else:
           self.rag_interface = RAGInterface()  # –°—Ç–∞—Ä—ã–π
   ```

4Ô∏è‚É£ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï:
   ```bash
   python -c "from src.submit.rag.vector_rag_interface import test_integration; test_integration()"
   ```

5Ô∏è‚É£ –í–´–ë–û–† –ú–û–î–ï–õ–ò:
   
   –î–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ):
   - "cointegrated/rubert-tiny2" (312MB, –±—ã—Å—Ç—Ä–∞—è) ‚≠ê
   - "cointegrated/LaBSE-en-ru" (490MB, –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è)
   - "ai-forever/sbert_large_nlu_ru" (1.7GB, —Ç–æ—á–Ω–∞—è)
   
   –ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã–µ:
   - "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
   - "intfloat/multilingual-e5-base"

6Ô∏è‚É£ –ù–ê–°–¢–†–û–ô–ö–ê –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò:
   
   –î–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤ (>10k –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤):
   ```python
   config = VectorRAGConfig(
       embedding_batch_size=64,  # –ë–æ–ª—å—à–µ –±–∞—Ç—á
       use_amp=True,             # Mixed precision
       compile_model=True,       # torch.compile
       optimize_on_save=True     # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏
   )
   ```

7Ô∏è‚É£ –ú–û–ù–ò–¢–û–†–ò–ù–ì:
   ```python
   # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
   stats = rag_interface.embedding_engine.get_stats()
   print(f"Cache hit rate: {stats['cache_hit_rate']:.1%}")
   ```

‚úÖ –ì–û–¢–û–í–û –ö –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Æ!

–ü—Ä–∏ –ø—Ä–æ–±–ª–µ–º–∞—Ö –ø—Ä–æ–≤–µ—Ä—å—Ç–µ:
- –í–µ—Ä—Å–∏—è transformers >= 4.30.0
- –í–µ—Ä—Å–∏—è torch >= 1.10.0
- –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ –¥–ª—è –º–æ–¥–µ–ª–∏ (min 2GB RAM)
    '''
    return guide


if __name__ == "__main__":
    print("\nüöÄ –í–ï–ö–¢–û–†–ù–´–ô –ü–û–ò–°–ö –î–õ–Ø GIGAMEMORY\n")
    print("–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ:")
    print("1. –ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç—ã")
    print("2. –ü–æ–∫–∞–∑–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –ø–æ —É—Å—Ç–∞–Ω–æ–≤–∫–µ")
    print("3. –ü–æ–∫–∞–∑–∞—Ç—å –∫–æ–¥ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏")
    
    choice = input("\n–í–∞—à –≤—ã–±–æ—Ä (1/2/3): ").strip()
    
    if choice == "1":
        test_integration()
    elif choice == "2":
        print(print_installation_guide())
    elif choice == "3":
        print(integrate_into_gigamemory())
    else:
        print("–ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä. –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é...")
        test_integration()
