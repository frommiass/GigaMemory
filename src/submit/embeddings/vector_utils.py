"""
–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏ –±–µ–Ω—á–º–∞—Ä–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
"""
import time
import logging
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path

from .improved_vector_search import ImprovedEmbeddingEngine, EmbeddingConfig
from .improved_vector_store import ImprovedVectorStore
from .vector_models import SimilarityMetric, PerformanceMetrics

logger = logging.getLogger(__name__)


def benchmark_performance(
    n_documents: int = 1000,
    model_name: str = "cointegrated/rubert-tiny2",
    batch_size: int = 32,
    k: int = 10,
    enable_amp: bool = True,
    use_cache: bool = True
) -> PerformanceMetrics:
    """
    –ë–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
    
    Args:
        n_documents: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
        k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        enable_amp: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å AMP
        use_cache: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—ç—à
    
    Returns:
        –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    """
    print("üöÄ –ë–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞")
    print("=" * 60)
    
    # –°–æ–∑–¥–∞–µ–º –¥–≤–∏–∂–æ–∫ –∏ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
    config = EmbeddingConfig(
        model_name=model_name,
        batch_size=batch_size,
        use_cache=use_cache,
        use_amp=enable_amp
    )
    
    engine = ImprovedEmbeddingEngine(config)
    store = ImprovedVectorStore(
        metric=SimilarityMetric.COSINE,
        enable_analytics=True
    )
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    texts = [f"–≠—Ç–æ —Ç–µ—Å—Ç–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –Ω–æ–º–µ—Ä {i} —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ–º" for i in range(n_documents)]
    
    # –ó–∞–º–µ—Ä—è–µ–º –≤—Ä–µ–º—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
    start = time.time()
    embeddings = engine.encode(texts, show_progress=True)
    encoding_time = time.time() - start
    
    encoding_speed = n_documents / encoding_time
    
    print(f"\nüìä –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ {n_documents} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:")
    print(f"   –í—Ä–µ–º—è: {encoding_time:.2f} —Å–µ–∫")
    print(f"   –°–∫–æ—Ä–æ—Å—Ç—å: {encoding_speed:.0f} –¥–æ–∫/—Å–µ–∫")
    print(f"   –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {engine.get_stats()}")
    
    # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
    start = time.time()
    for i, (text, embedding) in enumerate(zip(texts, embeddings)):
        store.add(
            doc_id=f"doc_{i}",
            vector=embedding,
            metadata={"category": i % 10},
            text=text
        )
    indexing_time = time.time() - start
    
    indexing_speed = n_documents / indexing_time
    
    print(f"\nüìä –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è {n_documents} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:")
    print(f"   –í—Ä–µ–º—è: {indexing_time:.2f} —Å–µ–∫")
    print(f"   –°–∫–æ—Ä–æ—Å—Ç—å: {indexing_speed:.0f} –¥–æ–∫/—Å–µ–∫")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫
    query_text = "–¥–æ–∫—É–º–µ–Ω—Ç —Å —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ–º"
    query_embedding = engine.encode(query_text)
    
    # –û–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫
    search_times = []
    for _ in range(10):  # –ù–µ—Å–∫–æ–ª—å–∫–æ –∏—Ç–µ—Ä–∞—Ü–∏–π –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        start = time.time()
        results = store.search(query_embedding, k=k)
        search_time = time.time() - start
        search_times.append(search_time)
    
    avg_search_time = np.mean(search_times)
    search_speed = 1.0 / avg_search_time
    
    print(f"\nüìä –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ (top-{k}):")
    print(f"   –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {avg_search_time*1000:.2f} –º—Å")
    print(f"   –°–∫–æ—Ä–æ—Å—Ç—å: {search_speed:.0f} –ø–æ–∏—Å–∫–æ–≤/—Å–µ–∫")
    print(f"   –¢–æ–ø-3 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞:")
    for r in results[:3]:
        print(f"     - {r.doc_id}: {r.score:.3f}")
    
    # –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫
    hybrid_times = []
    for _ in range(5):  # –ú–µ–Ω—å—à–µ –∏—Ç–µ—Ä–∞—Ü–∏–π –¥–ª—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        start = time.time()
        hybrid_results = store.hybrid_search(
            query_embedding, 
            query_text,
            k=k
        )
        hybrid_time = time.time() - start
        hybrid_times.append(hybrid_time)
    
    avg_hybrid_time = np.mean(hybrid_times)
    hybrid_speed = 1.0 / avg_hybrid_time
    
    print(f"\nüìä –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ (–≤–µ–∫—Ç–æ—Ä–Ω—ã–π + —Ç–µ–∫—Å—Ç–æ–≤—ã–π):")
    print(f"   –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {avg_hybrid_time*1000:.2f} –º—Å")
    print(f"   –°–∫–æ—Ä–æ—Å—Ç—å: {hybrid_speed:.0f} –ø–æ–∏—Å–∫–æ–≤/—Å–µ–∫")
    print(f"   –¢–æ–ø-3 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞:")
    for r in hybrid_results[:3]:
        print(f"     - {r.doc_id}: {r.score:.3f}")
    
    # –ê–Ω–∞–ª–∏—Ç–∏–∫–∞
    print(f"\nüìä –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞:")
    analytics = store.get_analytics()
    print(f"   –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {analytics.total_documents}")
    print(f"   –í—Å–µ–≥–æ –ø–æ–∏—Å–∫–æ–≤: {analytics.total_searches}")
    print(f"   –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –ø–æ–∏—Å–∫–∞: {analytics.avg_search_time:.3f} —Å–µ–∫")
    print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–π –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {len(analytics.unique_metadata_keys)}")
    
    # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    metrics = PerformanceMetrics(
        encoding_speed=encoding_speed,
        search_speed=search_speed,
        memory_efficiency=n_documents / (store.vectors.nbytes / 1024 / 1024),  # –¥–æ–∫/–ú–ë
        cache_efficiency=engine.get_stats().get('cache_hit_rate', 0.0),
        accuracy_score=0.0,  # –¢—Ä–µ–±—É–µ—Ç ground truth –¥–∞–Ω–Ω—ã—Ö
        latency_p95=np.percentile(search_times, 95) * 1000,  # –º—Å
        throughput=min(encoding_speed, search_speed)  # –æ–±—â–∞—è –ø—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å
    )
    
    print(f"\nüìä –ò—Ç–æ–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏:")
    print(f"   –°–∫–æ—Ä–æ—Å—Ç—å –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è: {metrics.encoding_speed:.0f} –¥–æ–∫/—Å–µ–∫")
    print(f"   –°–∫–æ—Ä–æ—Å—Ç—å –ø–æ–∏—Å–∫–∞: {metrics.search_speed:.0f} –ø–æ–∏—Å–∫–æ–≤/—Å–µ–∫")
    print(f"   –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–∞–º—è—Ç–∏: {metrics.memory_efficiency:.0f} –¥–æ–∫/–ú–ë")
    print(f"   –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∫—ç—à–∞: {metrics.cache_efficiency:.1%}")
    print(f"   P95 –∑–∞–¥–µ—Ä–∂–∫–∞: {metrics.latency_p95:.1f} –º—Å")
    print(f"   –û–±—â–∞—è –ø—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å: {metrics.throughput:.0f} –æ–ø/—Å–µ–∫")
    
    print("\n‚úÖ –ë–µ–Ω—á–º–∞—Ä–∫ –∑–∞–≤–µ—Ä—à–µ–Ω!")
    
    return metrics


def compare_models(
    models: List[str],
    n_documents: int = 100,
    batch_size: int = 16
) -> Dict[str, PerformanceMetrics]:
    """
    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
    
    Args:
        models: –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –º–æ–¥–µ–ª–µ–π
        n_documents: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
    
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
    """
    print("üîç –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞")
    print("=" * 60)
    
    results = {}
    
    for model_name in models:
        print(f"\nüß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å: {model_name}")
        print("-" * 40)
        
        try:
            metrics = benchmark_performance(
                n_documents=n_documents,
                model_name=model_name,
                batch_size=batch_size,
                enable_amp=True,
                use_cache=True
            )
            results[model_name] = metrics
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å –º–æ–¥–µ–ª—å—é {model_name}: {e}")
            continue
    
    # –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
    print(f"\nüìä –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞:")
    print("-" * 80)
    print(f"{'–ú–æ–¥–µ–ª—å':<30} {'–ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ':<12} {'–ü–æ–∏—Å–∫':<10} {'–ü–∞–º—è—Ç—å':<10} {'–ö—ç—à':<8}")
    print("-" * 80)
    
    for model_name, metrics in results.items():
        print(f"{model_name:<30} {metrics.encoding_speed:>8.0f} –¥–æ–∫/—Å {metrics.search_speed:>6.0f} –æ–ø/—Å "
              f"{metrics.memory_efficiency:>6.0f} –¥–æ–∫/–ú–ë {metrics.cache_efficiency:>6.1%}")
    
    return results


def stress_test(
    n_documents: int = 10000,
    n_searches: int = 1000,
    concurrent_searches: int = 10
) -> Dict[str, Any]:
    """
    –°—Ç—Ä–µ—Å—Å-—Ç–µ—Å—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
    
    Args:
        n_documents: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        n_searches: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        concurrent_searches: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–æ–∏—Å–∫–æ–≤
    
    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ç—Ä–µ—Å—Å-—Ç–µ—Å—Ç–∞
    """
    print("üí™ –°—Ç—Ä–µ—Å—Å-—Ç–µ—Å—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞")
    print("=" * 60)
    
    # –°–æ–∑–¥–∞–µ–º –¥–≤–∏–∂–æ–∫ –∏ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
    config = EmbeddingConfig(
        model_name="cointegrated/rubert-tiny2",
        batch_size=64,
        use_cache=True,
        use_amp=True
    )
    
    engine = ImprovedEmbeddingEngine(config)
    store = ImprovedVectorStore(
        metric=SimilarityMetric.COSINE,
        enable_analytics=True
    )
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    print(f"üìù –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º {n_documents} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
    texts = [f"–î–æ–∫—É–º–µ–Ω—Ç {i} —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ–º –∏ —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏" 
             for i in range(n_documents)]
    
    start = time.time()
    embeddings = engine.encode(texts, show_progress=True)
    encoding_time = time.time() - start
    
    # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
    print(f"üìö –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã...")
    start = time.time()
    for i, (text, embedding) in enumerate(zip(texts, embeddings)):
        store.add(
            doc_id=f"stress_doc_{i}",
            vector=embedding,
            metadata={"category": i % 100, "priority": i % 10},
            text=text
        )
    indexing_time = time.time() - start
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    print(f"üîç –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º {n_searches} –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤...")
    query_texts = [f"–ø–æ–∏—Å–∫ {i} —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏" for i in range(n_searches)]
    query_embeddings = engine.encode(query_texts, show_progress=True)
    
    # –°—Ç—Ä–µ—Å—Å-—Ç–µ—Å—Ç –ø–æ–∏—Å–∫–∞
    print(f"‚ö° –ó–∞–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–µ—Å—Å-—Ç–µ—Å—Ç ({n_searches} –ø–æ–∏—Å–∫–æ–≤)...")
    
    search_times = []
    start = time.time()
    
    for i, (query_text, query_embedding) in enumerate(zip(query_texts, query_embeddings)):
        search_start = time.time()
        
        # –ß–µ—Ä–µ–¥—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã –ø–æ–∏—Å–∫–∞
        if i % 3 == 0:
            results = store.search(query_embedding, k=10)
        elif i % 3 == 1:
            results = store.search(query_embedding, k=10, filter_metadata={"category": i % 100})
        else:
            results = store.hybrid_search(query_embedding, query_text, k=10)
        
        search_time = time.time() - search_start
        search_times.append(search_time)
        
        if (i + 1) % 100 == 0:
            print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i + 1}/{n_searches} –ø–æ–∏—Å–∫–æ–≤")
    
    total_time = time.time() - start
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    results = {
        'total_documents': n_documents,
        'total_searches': n_searches,
        'total_time': total_time,
        'avg_search_time': np.mean(search_times),
        'min_search_time': np.min(search_times),
        'max_search_time': np.max(search_times),
        'p95_search_time': np.percentile(search_times, 95),
        'p99_search_time': np.percentile(search_times, 99),
        'searches_per_second': n_searches / total_time,
        'encoding_time': encoding_time,
        'indexing_time': indexing_time,
        'memory_usage_mb': store.vectors.nbytes / 1024 / 1024,
        'analytics': store.get_analytics()
    }
    
    print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ç—Ä–µ—Å—Å-—Ç–µ—Å—Ç–∞:")
    print(f"   –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {results['total_documents']:,}")
    print(f"   –í—Å–µ–≥–æ –ø–æ–∏—Å–∫–æ–≤: {results['total_searches']:,}")
    print(f"   –û–±—â–µ–µ –≤—Ä–µ–º—è: {results['total_time']:.2f} —Å–µ–∫")
    print(f"   –ü–æ–∏—Å–∫–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É: {results['searches_per_second']:.0f}")
    print(f"   –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –ø–æ–∏—Å–∫–∞: {results['avg_search_time']*1000:.2f} –º—Å")
    print(f"   P95 –≤—Ä–µ–º—è –ø–æ–∏—Å–∫–∞: {results['p95_search_time']*1000:.2f} –º—Å")
    print(f"   P99 –≤—Ä–µ–º—è –ø–æ–∏—Å–∫–∞: {results['p99_search_time']*1000:.2f} –º—Å")
    print(f"   –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏: {results['memory_usage_mb']:.1f} –ú–ë")
    
    print("\n‚úÖ –°—Ç—Ä–µ—Å—Å-—Ç–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω!")
    
    return results


def optimize_store_config(
    store: ImprovedVectorStore,
    test_queries: List[np.ndarray],
    optimization_target: str = "speed"
) -> Dict[str, Any]:
    """
    –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
    
    Args:
        store: –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        test_queries: –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        optimization_target: –¶–µ–ª—å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ ("speed", "accuracy", "memory")
    
    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    """
    print(f"‚öôÔ∏è –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ (—Ü–µ–ª—å: {optimization_target})")
    print("=" * 60)
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    configs = [
        {"k": 5, "rerank": False},
        {"k": 10, "rerank": False},
        {"k": 5, "rerank": True},
        {"k": 10, "rerank": True},
    ]
    
    results = {}
    
    for config in configs:
        config_name = f"k={config['k']}, rerank={config['rerank']}"
        print(f"\nüß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é: {config_name}")
        
        search_times = []
        for query in test_queries[:10]:  # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –ø–µ—Ä–≤—ã—Ö 10 –∑–∞–ø—Ä–æ—Å–∞—Ö
            start = time.time()
            results_list = store.search(query, **config)
            search_time = time.time() - start
            search_times.append(search_time)
        
        avg_time = np.mean(search_times)
        results[config_name] = {
            'avg_search_time': avg_time,
            'searches_per_second': 1.0 / avg_time,
            'config': config
        }
        
        print(f"   –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {avg_time*1000:.2f} –º—Å")
        print(f"   –ü–æ–∏—Å–∫–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É: {1.0/avg_time:.0f}")
    
    # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    if optimization_target == "speed":
        best_config = min(results.items(), key=lambda x: x[1]['avg_search_time'])
    else:
        # –î–ª—è –¥—Ä—É–≥–∏—Ö —Ü–µ–ª–µ–π –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        best_config = min(results.items(), key=lambda x: x[1]['avg_search_time'])
    
    print(f"\nüèÜ –õ—É—á—à–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {best_config[0]}")
    print(f"   –í—Ä–µ–º—è –ø–æ–∏—Å–∫–∞: {best_config[1]['avg_search_time']*1000:.2f} –º—Å")
    print(f"   –°–∫–æ—Ä–æ—Å—Ç—å: {best_config[1]['searches_per_second']:.0f} –ø–æ–∏—Å–∫–æ–≤/—Å–µ–∫")
    
    return {
        'best_config': best_config[1]['config'],
        'best_performance': best_config[1],
        'all_results': results
    }


if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –±–µ–Ω—á–º–∞—Ä–∫
    benchmark_performance()
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –º–æ–∂–Ω–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    # models = ["cointegrated/rubert-tiny2", "DeepPavlov/rubert-base-cased"]
    # compare_models(models, n_documents=50)
