#!/usr/bin/env python3
"""
–ò–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ—Å—Ç VectorRAGInterface –±–µ–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –æ—Ç –¥—Ä—É–≥–∏—Ö –º–æ–¥—É–ª–µ–π
"""
import sys
import os
import torch
import numpy as np
import logging
from pathlib import Path
from typing import List, Dict, Optional, Tuple, Any
from datetime import datetime
from dataclasses import dataclass, field
from enum import Enum
import hashlib
import json
import pickle
import gzip
import time
from collections import defaultdict, deque
import warnings
from abc import ABC, abstractmethod
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ö–æ–ø–∏—Ä—É–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–ª–∞—Å—Å—ã –Ω–∞–ø—Ä—è–º—É—é –¥–ª—è –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
class PoolingStrategy(Enum):
    """–°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø—É–ª–∏–Ω–≥–∞ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    MEAN = "mean"
    MAX = "max"
    CLS = "cls"
    MEAN_MAX = "mean_max"
    WEIGHTED_MEAN = "weighted_mean"

class SimilarityMetric(Enum):
    """–ú–µ—Ç—Ä–∏–∫–∏ —Å—Ö–æ–¥—Å—Ç–≤–∞"""
    COSINE = "cosine"
    EUCLIDEAN = "euclidean"
    DOT_PRODUCT = "dot"
    MANHATTAN = "manhattan"
    ANGULAR = "angular"

@dataclass
class EmbeddingConfig:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –¥–≤–∏–∂–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    model_name: str = "cointegrated/rubert-tiny2"
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    max_length: int = 512
    batch_size: int = 32
    normalize: bool = True
    pooling_strategy: PoolingStrategy = PoolingStrategy.MEAN
    cache_dir: Optional[str] = None
    use_cache: bool = True
    use_amp: bool = True
    num_workers: int = 4
    prefetch_batches: int = 2
    warmup_steps: int = 0
    compile_model: bool = False
    quantize_model: bool = False

@dataclass
class VectorDocument:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–º —Ö—Ä–∞–Ω–∏–ª–∏—â–µ"""
    id: str
    vector: np.ndarray
    metadata: Dict[str, Any] = field(default_factory=dict)
    text: Optional[str] = None
    timestamp: datetime = field(default_factory=datetime.now)
    access_count: int = 0
    last_accessed: Optional[datetime] = None

@dataclass
class SearchResult:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞"""
    doc_id: str
    score: float
    metadata: Dict[str, Any]
    text: Optional[str] = None
    rank: int = 0
    explanation: Optional[Dict[str, Any]] = None

@dataclass
class VectorStoreStats:
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞"""
    total_documents: int = 0
    total_searches: int = 0
    avg_search_time: float = 0.0
    cache_hit_rate: float = 0.0
    memory_usage_mb: float = 0.0
    popular_queries: Dict[str, int] = field(default_factory=dict)
    access_patterns: Dict[str, int] = field(default_factory=dict)
    top_accessed_docs: List[Dict[str, Any]] = field(default_factory=list)
    unique_metadata_keys: List[str] = field(default_factory=list)

class MockEmbeddingEngine:
    """–ú–æ–∫ –¥–≤–∏–∂–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    
    def __init__(self, config: EmbeddingConfig):
        self.config = config
        self.stats = {
            'total_encoded': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'encoding_time': 0.0,
            'last_batch_time': 0.0
        }
        logger.info(f"MockEmbeddingEngine —Å–æ–∑–¥–∞–Ω: {config.model_name}")
    
    def encode(self, texts):
        """–ú–æ–∫ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã"""
        if isinstance(texts, str):
            texts = [texts]
            single_text = True
        else:
            single_text = False
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ 312 (rubert-tiny2)
        embeddings = np.random.randn(len(texts), 312).astype(np.float32)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
        if self.config.normalize:
            embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)
        
        self.stats['total_encoded'] += len(texts)
        
        if single_text:
            return embeddings[0]
        return embeddings
    
    def get_stats(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        stats = self.stats.copy()
        stats['cache_size'] = 0
        stats['cache_hit_rate'] = 0.0
        return stats

class MockVectorStore:
    """–ú–æ–∫ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    
    def __init__(self, metric=SimilarityMetric.COSINE, enable_analytics=True):
        self.metric = metric
        self.enable_analytics = enable_analytics
        self.documents = []
        self.vectors = None
        self.id_to_index = {}
        
        if enable_analytics:
            self.analytics = {
                'total_searches': 0,
                'avg_search_time': 0.0
            }
        
        logger.info(f"MockVectorStore —Å–æ–∑–¥–∞–Ω: metric={metric.value}")
    
    def add(self, doc_id, vector, metadata=None, text=None):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        doc = VectorDocument(
            id=doc_id,
            vector=vector,
            metadata=metadata or {},
            text=text
        )
        
        index = len(self.documents)
        self.documents.append(doc)
        self.id_to_index[doc_id] = index
        
        if self.vectors is None:
            self.vectors = vector.reshape(1, -1)
        else:
            self.vectors = np.vstack([self.vectors, vector])
        
        return True
    
    def search(self, query_vector, k=5, threshold=None):
        """–ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫"""
        if not self.documents:
            return []
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ
        scores = np.dot(self.vectors, query_vector)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–æ—Ä–æ–≥
        if threshold is not None:
            mask = scores >= threshold
            scores = scores[mask]
            valid_indices = [i for i, m in enumerate(mask) if m]
        else:
            valid_indices = list(range(len(self.documents)))
        
        if len(valid_indices) == 0:
            return []
        
        # –ù–∞—Ö–æ–¥–∏–º —Ç–æ–ø-k
        k = min(k, len(valid_indices))
        top_indices = np.argpartition(scores, -k)[-k:]
        top_indices = top_indices[np.argsort(scores[top_indices])[::-1]]
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results = []
        for rank, idx in enumerate(top_indices):
            doc_idx = valid_indices[idx]
            doc = self.documents[doc_idx]
            
            results.append(SearchResult(
                doc_id=doc.id,
                score=float(scores[idx]),
                metadata=doc.metadata,
                text=doc.text,
                rank=rank
            ))
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∞–Ω–∞–ª–∏—Ç–∏–∫—É
        if self.enable_analytics:
            self.analytics['total_searches'] += 1
        
        return results
    
    def hybrid_search(self, query_vector, query_text, k=5, vector_weight=0.7, text_weight=0.3):
        """–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫"""
        # –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è - –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
        return self.search(query_vector, k)
    
    def size(self):
        """–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        return len(self.documents)
    
    def get_analytics(self):
        """–ê–Ω–∞–ª–∏—Ç–∏–∫–∞"""
        if not self.enable_analytics:
            return VectorStoreStats()
        
        return VectorStoreStats(
            total_documents=len(self.documents),
            total_searches=self.analytics['total_searches'],
            avg_search_time=self.analytics['avg_search_time']
        )
    
    def save(self, filepath):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ (–º–æ–∫)"""
        logger.info(f"MockVectorStore —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {filepath}")
    
    def load(self, filepath):
        """–ó–∞–≥—Ä—É–∑–∫–∞ (–º–æ–∫)"""
        logger.info(f"MockVectorStore –∑–∞–≥—Ä—É–∂–µ–Ω: {filepath}")

class MockVectorRAGInterface:
    """–ú–æ–∫ VectorRAGInterface –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    
    def __init__(self, model_name="cointegrated/rubert-tiny2", use_gpu=True, enable_hybrid_search=True):
        self.embedding_engine = MockEmbeddingEngine(
            EmbeddingConfig(
                model_name=model_name,
                device="cuda" if use_gpu else "cpu",
                batch_size=32,
                use_cache=True
            )
        )
        
        self.stores = {}
        self.enable_hybrid = enable_hybrid_search
        self.dialogues = {}
        
        logger.info(f"MockVectorRAGInterface –≥–æ—Ç–æ–≤: {model_name}")
    
    def add_dialogue(self, dialogue_id, messages):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–∏–∞–ª–æ–≥–∞"""
        self.dialogues[dialogue_id] = messages
        
        if dialogue_id not in self.stores:
            self.stores[dialogue_id] = MockVectorStore(
                metric=SimilarityMetric.COSINE,
                enable_analytics=True
            )
        
        self._index_messages(dialogue_id, messages)
    
    def get_relevant_context(self, query, dialogue_id, top_k=5):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        if dialogue_id not in self.stores:
            return ""
        
        query_embedding = self.embedding_engine.encode(query)
        store = self.stores[dialogue_id]
        
        if self.enable_hybrid:
            results = store.hybrid_search(query_vector=query_embedding, query_text=query, k=top_k)
        else:
            results = store.search(query_vector=query_embedding, k=top_k, threshold=0.7)
        
        if not results:
            return ""
        
        context_parts = ["–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞:"]
        for result in results:
            if result.text:
                context_parts.append(f"- {result.text}")
        
        return "\n".join(context_parts)
    
    def search(self, query, dialogue_id, **kwargs):
        """–ü–æ–∏—Å–∫"""
        if dialogue_id not in self.stores:
            return []
        
        query_embedding = self.embedding_engine.encode(query)
        store = self.stores[dialogue_id]
        
        results = store.search(
            query_vector=query_embedding,
            k=kwargs.get('top_k', 5),
            threshold=kwargs.get('threshold', 0.7)
        )
        
        return [
            {
                'id': r.doc_id,
                'score': r.score,
                'text': r.text,
                'metadata': r.metadata
            }
            for r in results
        ]
    
    def process_message(self, message, dialogue_id, role="user"):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è"""
        if dialogue_id not in self.dialogues:
            self.dialogues[dialogue_id] = []
        
        self.dialogues[dialogue_id].append({
            'role': role,
            'content': message
        })
        
        if role == "assistant":
            self._index_single_message(dialogue_id, message, role)
        
        if role == "user":
            context = self.get_relevant_context(message, dialogue_id)
            if context:
                return f"{context}\n\n–í–æ–ø—Ä–æ—Å: {message}"
        
        return message
    
    def _index_messages(self, dialogue_id, messages):
        """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π"""
        store = self.stores[dialogue_id]
        
        chunk_size = 3
        chunks = []
        
        for i in range(0, len(messages), chunk_size):
            chunk = messages[i:i+chunk_size]
            chunk_text = "\n".join([
                f"{msg.get('role', 'unknown')}: {msg.get('content', '')}" 
                for msg in chunk
            ])
            chunks.append(chunk_text)
        
        if chunks:
            embeddings = self.embedding_engine.encode(chunks)
            
            for i, (chunk_text, embedding) in enumerate(zip(chunks, embeddings)):
                store.add(
                    doc_id=f"chunk_{dialogue_id}_{i}",
                    vector=embedding,
                    text=chunk_text,
                    metadata={'chunk_index': i, 'dialogue_id': dialogue_id}
                )
    
    def _index_single_message(self, dialogue_id, message, role):
        """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –æ–¥–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è"""
        if dialogue_id not in self.stores:
            self.stores[dialogue_id] = MockVectorStore(metric=SimilarityMetric.COSINE)
        
        store = self.stores[dialogue_id]
        embedding = self.embedding_engine.encode(message)
        
        store.add(
            doc_id=f"msg_{dialogue_id}_{datetime.now().timestamp()}",
            vector=embedding,
            text=f"{role}: {message}",
            metadata={'role': role, 'timestamp': datetime.now().isoformat()}
        )
    
    def save(self, path="./rag_indices"):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ"""
        save_dir = Path(path)
        save_dir.mkdir(exist_ok=True, parents=True)
        
        for dialogue_id, store in self.stores.items():
            store.save(str(save_dir / f"{dialogue_id}.idx"))
        
        with open(save_dir / "dialogues.json", 'w') as f:
            json.dump(self.dialogues, f)
        
        logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(self.stores)} –∏–Ω–¥–µ–∫—Å–æ–≤")
    
    def load(self, path="./rag_indices"):
        """–ó–∞–≥—Ä—É–∑–∫–∞"""
        load_dir = Path(path)
        
        if not load_dir.exists():
            return
        
        dialogues_file = load_dir / "dialogues.json"
        if dialogues_file.exists():
            with open(dialogues_file, 'r') as f:
                self.dialogues = json.load(f)
        
        for idx_file in load_dir.glob("*.idx"):
            dialogue_id = idx_file.stem
            store = MockVectorStore(metric=SimilarityMetric.COSINE)
            store.load(str(idx_file))
            self.stores[dialogue_id] = store
        
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.stores)} –∏–Ω–¥–µ–∫—Å–æ–≤")

def test_mock_vector_rag():
    """–¢–µ—Å—Ç –º–æ–∫ VectorRAGInterface"""
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ MockVectorRAGInterface")
    print("=" * 50)
    
    try:
        # –°–æ–∑–¥–∞–µ–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
        print("\n1Ô∏è‚É£ –°–æ–∑–¥–∞–Ω–∏–µ MockVectorRAGInterface...")
        rag = MockVectorRAGInterface(
            model_name="cointegrated/rubert-tiny2",
            use_gpu=False,  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ CPU –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            enable_hybrid_search=True
        )
        print(f"‚úÖ –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ")
        print(f"   –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫: {'–í–∫–ª—é—á–µ–Ω' if rag.enable_hybrid else '–û—Ç–∫–ª—é—á–µ–Ω'}")
        
        # –¢–µ—Å—Ç–æ–≤—ã–π –¥–∏–∞–ª–æ–≥
        print("\n2Ô∏è‚É£ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        dialogue_id = "test_dialogue_001"
        messages = [
            {"role": "user", "content": "–ü—Ä–∏–≤–µ—Ç! –ú–µ–Ω—è –∑–æ–≤—É—Ç –ê–ª–µ–∫—Å–µ–π, —è –∏–∑ –ú–æ—Å–∫–≤—ã."},
            {"role": "assistant", "content": "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ, –ê–ª–µ–∫—Å–µ–π! –†–∞–¥ –ø–æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è. –ö–∞–∫ –¥–µ–ª–∞ –≤ –ú–æ—Å–∫–≤–µ?"},
            {"role": "user", "content": "–û—Ç–ª–∏—á–Ω–æ! –Ø —Ä–∞–±–æ—Ç–∞—é –¥–∞—Ç–∞-—Å–∞–π–µ–Ω—Ç–∏—Å—Ç–æ–º –≤ –°–±–µ—Ä–µ."},
            {"role": "assistant", "content": "–ó–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–æ! Data Science - –æ—á–µ–Ω—å –∏–Ω—Ç–µ—Ä–µ—Å–Ω–∞—è –æ–±–ª–∞—Å—Ç—å."},
            {"role": "user", "content": "–£ –º–µ–Ω—è –µ—Å—Ç—å –∫–æ—à–∫–∞ –ú—É—Ä–∫–∞ –∏ —Å–æ–±–∞–∫–∞ –†–µ–∫—Å."},
            {"role": "assistant", "content": "–ó–¥–æ—Ä–æ–≤–æ! –ú—É—Ä–∫–∞ –∏ –†–µ–∫—Å - –ø—Ä–µ–∫—Ä–∞—Å–Ω—ã–µ –∏–º–µ–Ω–∞ –¥–ª—è –ø–∏—Ç–æ–º—Ü–µ–≤."},
            {"role": "user", "content": "–Ø —É–≤–ª–µ–∫–∞—é—Å—å –º–∞—à–∏–Ω–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º –∏ –Ω–µ–π—Ä–æ—Å–µ—Ç—è–º–∏."},
            {"role": "assistant", "content": "–û—Ç–ª–∏—á–Ω–æ–µ —É–≤–ª–µ—á–µ–Ω–∏–µ! ML –∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ —Å–µ–π—á–∞—Å –æ—á–µ–Ω—å –∞–∫—Ç—É–∞–ª—å–Ω—ã."},
        ]
        print(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(messages)} —Å–æ–æ–±—â–µ–Ω–∏–π")
        
        # –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–∏–∞–ª–æ–≥–∞
        print("\n3Ô∏è‚É£ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–∏–∞–ª–æ–≥–∞...")
        rag.add_dialogue(dialogue_id, messages)
        print(f"‚úÖ –î–∏–∞–ª–æ–≥ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å–æ–∑–¥–∞–Ω–æ
        if dialogue_id in rag.stores:
            store = rag.stores[dialogue_id]
            print(f"   –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ: {store.size()}")
        
        # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        print("\n4Ô∏è‚É£ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∏—Å–∫–∞...")
        test_queries = [
            "–ö–∞–∫ –º–µ–Ω—è –∑–æ–≤—É—Ç?",
            "–ì–¥–µ —è —Ä–∞–±–æ—Ç–∞—é?", 
            "–ö–∞–∫–∏–µ —É –º–µ–Ω—è –ø–∏—Ç–æ–º—Ü—ã?",
            "–ß–µ–º —è —É–≤–ª–µ–∫–∞—é—Å—å?",
            "–û—Ç–∫—É–¥–∞ —è?"
        ]
        
        for i, query in enumerate(test_queries, 1):
            print(f"\n   {i}. –í–æ–ø—Ä–æ—Å: {query}")
            
            # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
            context = rag.get_relevant_context(query, dialogue_id, top_k=2)
            
            if context:
                print("   üìö –ù–∞–π–¥–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç:")
                lines = context.split('\n')[1:]  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                for line in lines[:2]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 2 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                    if line.strip():
                        print(f"      {line[:80]}...")
            else:
                print("   ‚ùå –ö–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        # –¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
        print("\n5Ô∏è‚É£ –¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è...")
        new_question = "–ö–∞–∫ –∑–æ–≤—É—Ç –º–æ—é –∫–æ—à–∫—É –∏ –≥–¥–µ —è —Ä–∞–±–æ—Ç–∞—é?"
        enhanced = rag.process_message(new_question, dialogue_id, role="user")
        
        print(f"   –ò—Å—Ö–æ–¥–Ω—ã–π –≤–æ–ø—Ä–æ—Å: {new_question}")
        print(f"   –û–±–æ–≥–∞—â–µ–Ω–Ω—ã–π –≤–æ–ø—Ä–æ—Å:")
        print(f"   {enhanced[:200]}...")
        
        # –¢–µ—Å—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ –∑–∞–≥—Ä—É–∑–∫–∏
        print("\n6Ô∏è‚É£ –¢–µ—Å—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è/–∑–∞–≥—Ä—É–∑–∫–∏...")
        test_save_path = "./test_vector_indices"
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        rag.save(test_save_path)
        print(f"‚úÖ –ò–Ω–¥–µ–∫—Å—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {test_save_path}")
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º
        rag2 = MockVectorRAGInterface()
        rag2.load(test_save_path)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥—Ä—É–∑–∫—É
        test_query = "–ø–∏—Ç–æ–º—Ü—ã"
        results = rag2.search(test_query, dialogue_id, top_k=1)
        
        if results:
            print(f"‚úÖ –ò–Ω–¥–µ–∫—Å—ã —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
            print(f"   –ù–∞–π–¥–µ–Ω–æ: {results[0]['text'][:50]}...")
        else:
            print("‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω–¥–µ–∫—Å–æ–≤")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print("\n7Ô∏è‚É£ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∏—Å—Ç–µ–º—ã...")
        if dialogue_id in rag.stores:
            store = rag.stores[dialogue_id]
            analytics = store.get_analytics()
            print(f"   –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å–µ: {analytics.total_documents}")
            print(f"   –í—Å–µ–≥–æ –ø–æ–∏—Å–∫–æ–≤: {analytics.total_searches}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        emb_stats = rag.embedding_engine.get_stats()
        print(f"   –ó–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤: {emb_stats['total_encoded']}")
        
        # –û—á–∏—Å—Ç–∫–∞
        print("\n8Ô∏è‚É£ –û—á–∏—Å—Ç–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤...")
        if Path(test_save_path).exists():
            import shutil
            shutil.rmtree(test_save_path)
            print("‚úÖ –¢–µ—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã —É–¥–∞–ª–µ–Ω—ã")
        
        print("\nüéâ –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
        return True
        
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    print("üöÄ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –í–ï–ö–¢–û–†–ù–û–ì–û RAG –î–õ–Ø GIGAMEMORY (–ú–û–ö)")
    print("=" * 60)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å PyTorch
    print(f"PyTorch –≤–µ—Ä—Å–∏—è: {torch.__version__}")
    print(f"CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.cuda.is_available()}")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç
    test_passed = test_mock_vector_rag()
    
    # –ò—Ç–æ–≥–∏
    print("\nüìä –ò–¢–û–ì–ò –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø")
    print("=" * 30)
    print(f"MockVectorRAGInterface: {'‚úÖ –ü–†–û–ô–î–ï–ù' if test_passed else '‚ùå –ü–†–û–í–ê–õ–ï–ù'}")
    
    if test_passed:
        print("\nüéâ –¢–ï–°–¢ –ü–†–û–ô–î–ï–ù –£–°–ü–ï–®–ù–û!")
        print("–õ–æ–≥–∏–∫–∞ VectorRAGInterface —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!")
        print("–ì–æ—Ç–æ–≤–æ –∫ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏!")
        return True
    else:
        print("\n‚ùå –¢–ï–°–¢ –ü–†–û–í–ê–õ–ï–ù")
        print("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –æ—à–∏–±–∫–∏ –≤—ã—à–µ")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
