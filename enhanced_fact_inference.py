#!/usr/bin/env python
"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π FactBasedRAGEngine –∏ FactBasedQuestionClassifier
"""
import argparse
import json
import sys
import re
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º
sys.path.append(str(Path(__file__).parent / "src"))

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–∞—à–∏ –º–æ–¥—É–ª–∏
try:
    from extraction.fact_models import FactType, Fact, FactConfidence, FactRelation
    from questions.fact_based_classifier import FactBasedQuestionClassifier
    from rag.fact_based_rag import FactBasedRAGEngine
    from extraction.fact_database import FactDatabase
except ImportError as e:
    print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥—É–ª–∏: {e}")
    print("–°–æ–∑–¥–∞–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é...")
    
    # –°–æ–∑–¥–∞–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã
    class FactType:
        WORK_OCCUPATION = "work_occupation"
        PERSONAL_NAME = "personal_name"
        PERSONAL_AGE = "personal_age"
        FAMILY_STATUS = "family_status"
        SPORT_TYPE = "sport_type"
        PET_TYPE = "pet_type"
        TRANSPORT_CAR_BRAND = "transport_car_brand"
        DRINK_COFFEE = "drink_coffee"
        FINANCE_INCOME = "finance_income"
        EDUCATION_INSTITUTION = "education_institution"
        HOBBY_ACTIVITY = "hobby_activity"
        HEALTH_CONDITION = "health_condition"
        PROPERTY_TYPE = "property_type"
        CONTACT_PHONE = "contact_phone"
        CONTACT_EMAIL = "contact_email"
    
    class FactConfidence:
        def __init__(self, score: float):
            self.score = score
    
    class FactRelation:
        HAS = "has"
        IS = "is"
        WORKS_AS = "works_as"
        LIVES_IN = "lives_in"
        OWNS = "owns"
        LIKES = "likes"
    
    class Fact:
        def __init__(self, type: str, subject: str, relation: str, object: str, 
                     confidence: FactConfidence, session_id: str, dialogue_id: str):
            self.type = type
            self.subject = subject
            self.relation = relation
            self.object = object
            self.confidence = confidence
            self.session_id = session_id
            self.dialogue_id = dialogue_id
        
        def to_natural_text(self) -> str:
            return f"{self.subject} {self.relation} {self.object}"
    
    class FactDatabase:
        def __init__(self):
            self.facts = []
        
        def add_fact(self, fact: Fact):
            self.facts.append(fact)
        
        def query_facts(self, dialogue_id: str, fact_type: str = None, 
                       min_confidence: float = 0.0, query: str = None) -> List[Fact]:
            results = []
            for fact in self.facts:
                if fact.dialogue_id == dialogue_id:
                    if fact_type and fact.type != fact_type:
                        continue
                    if fact.confidence.score < min_confidence:
                        continue
                    if query and query.lower() not in fact.object.lower():
                        continue
                    results.append(fact)
            return results
    
    class FactBasedQuestionClassifier:
        def __init__(self):
            self.question_patterns = {
                FactType.WORK_OCCUPATION: [
                    r'–∫–µ–º\s+(?:—è\s+)?—Ä–∞–±–æ—Ç–∞',
                    r'(?:–º–æ—è\s+)?(?:–ø—Ä–æ—Ñ–µ—Å—Å–∏—è|—Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å)',
                    r'—á–µ–º\s+(?:—è\s+)?–∑–∞–Ω–∏–º–∞',
                ],
                FactType.PERSONAL_NAME: [
                    r'–∫–∞–∫\s+(?:–º–µ–Ω—è\s+)?–∑–æ–≤—É—Ç',
                    r'(?:–º–æ–µ|–º–æ—ë)\s+–∏–º—è',
                    r'–∫—Ç–æ\s+—è(?:\s+—Ç–∞–∫–æ–π)?',
                ],
                FactType.PERSONAL_AGE: [
                    r'—Å–∫–æ–ª—å–∫–æ\s+(?:–º–Ω–µ\s+)?–ª–µ—Ç',
                    r'(?:–º–æ–π\s+)?–≤–æ–∑—Ä–∞—Å—Ç',
                    r'–∫–æ–≥–¥–∞\s+(?:—è\s+)?—Ä–æ–¥–∏–ª',
                ],
                FactType.SPORT_TYPE: [
                    r'(?:–∫–∞–∫–∏–º\s+)?—Å–ø–æ—Ä—Ç–æ–º\s+(?:—è\s+)?(?:–∑–∞–Ω–∏–º–∞—é—Å—å|—É–≤–ª–µ–∫–∞—é—Å—å)',
                    r'(?:–º–æ–∏\s+)?(?:—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏|—Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ\s+–Ω–∞–≥—Ä—É–∑–∫–∏)',
                ],
                FactType.PET_TYPE: [
                    r'(?:–∫–∞–∫–∏–µ\s+)?(?:—É\s+–º–µ–Ω—è\s+)?(?:–ø–∏—Ç–æ–º—Ü—ã|–∂–∏–≤–æ—Ç–Ω—ã–µ)',
                    r'–µ—Å—Ç—å\s+–ª–∏\s+(?:—É\s+–º–µ–Ω—è\s+)?(?:–∫–æ—Ç|–∫–æ—à–∫–∞|—Å–æ–±–∞–∫–∞)',
                    r'(?:–∫–∞–∫–∞—è\s+)?(?:—É\s+–º–µ–Ω—è\s+)?(?:—Å–æ–±–∞–∫–∞|–∫–æ—à–∫–∞)',
                ],
                FactType.TRANSPORT_CAR_BRAND: [
                    r'(?:–∫–∞–∫–∞—è\s+)?(?:—É\s+–º–µ–Ω—è\s+)?(?:–º–∞—à–∏–Ω–∞|–∞–≤—Ç–æ|—Ç–∞—á–∫–∞)',
                    r'(?:–º–∞—Ä–∫–∞|–±—Ä–µ–Ω–¥)\s+(?:–º–æ–µ–π\s+)?(?:–º–∞—à–∏–Ω—ã|–∞–≤—Ç–æ)',
                    r'–Ω–∞\s+—á–µ–º\s+(?:—è\s+)?(?:–µ–∑–∂—É|–∫–∞—Ç–∞—é—Å—å)',
                ],
                FactType.DRINK_COFFEE: [
                    r'(?:–∫–∞–∫–æ–π\s+)?–∫–æ—Ñ–µ\s+(?:—è\s+)?(?:–ø—å—é|–ª—é–±–ª—é)',
                    r'(?:–ø—å—é\s+–ª–∏\s+—è\s+)?–∫–æ—Ñ–µ',
                ],
                FactType.FINANCE_INCOME: [
                    r'(?:–º–æ–π\s+)?(?:–¥–æ—Ö–æ–¥|–∑–∞—Ä–∞–±–æ—Ç–æ–∫)',
                    r'—Å–∫–æ–ª—å–∫–æ\s+(?:—è\s+)?(?:–ø–æ–ª—É—á–∞—é|–∏–º–µ—é)',
                ],
                FactType.EDUCATION_INSTITUTION: [
                    r'–≥–¥–µ\s+(?:—è\s+)?(?:—É—á–∏–ª—Å—è|—É—á–∏–ª–∞—Å—å|—É—á—É—Å—å)',
                    r'(?:–º–æ–π\s+)?(?:—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç|–∏–Ω—Å—Ç–∏—Ç—É—Ç|–≤—É–∑)',
                    r'(?:–∫–∞–∫–æ–µ\s+)?(?:—É\s+–º–µ–Ω—è\s+)?–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ',
                ],
                FactType.HOBBY_ACTIVITY: [
                    r'(?:–º–æ–∏\s+)?(?:—Ö–æ–±–±–∏|—É–≤–ª–µ—á–µ–Ω–∏—è|–∏–Ω—Ç–µ—Ä–µ—Å—ã)',
                    r'—á–µ–º\s+(?:—è\s+)?(?:—É–≤–ª–µ–∫–∞—é—Å—å|–∏–Ω—Ç–µ—Ä–µ—Å—É—é—Å—å)',
                    r'(?:–º–æ–µ|–º–æ—ë)\s+(?:–ª—é–±–∏–º–æ–µ\s+)?(?:–∑–∞–Ω—è—Ç–∏–µ|–¥–µ–ª–æ)',
                ],
                FactType.HEALTH_CONDITION: [
                    r'(?:–º–æ–µ|–º–æ—ë)\s+(?:–∑–¥–æ—Ä–æ–≤—å–µ|—Å–∞–º–æ—á—É–≤—Å—Ç–≤–∏–µ)',
                    r'(?:—á–µ–º\s+)?(?:—è\s+)?(?:–±–æ–ª–µ—é|–±–æ–ª–µ–ª)',
                    r'(?:–º–æ–∏\s+)?(?:–±–æ–ª–µ–∑–Ω–∏|–∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—è)',
                ],
                FactType.PROPERTY_TYPE: [
                    r'(?:–≥–¥–µ|–∫–∞–∫)\s+(?:—è\s+)?–∂–∏–≤—É',
                    r'(?:–º–æ—è\s+)?(?:–∫–≤–∞—Ä—Ç–∏—Ä–∞|–¥–æ–º|–∂–∏–ª—å–µ)',
                    r'(?:—Å–∫–æ–ª—å–∫–æ\s+)?–∫–æ–º–Ω–∞—Ç',
                ],
                FactType.CONTACT_PHONE: [
                    r'(?:–º–æ–π\s+)?(?:–Ω–æ–º–µ—Ä|—Ç–µ–ª–µ—Ñ–æ–Ω)',
                    r'(?:–∫–∞–∫\s+)?(?:—Å–æ\s+–º–Ω–æ–π\s+)?(?:—Å–≤—è–∑–∞—Ç—å—Å—è|–ø–æ–∑–≤–æ–Ω–∏—Ç—å)',
                ],
                FactType.CONTACT_EMAIL: [
                    r'(?:–º–æ–π\s+)?(?:email|–ø–æ—á—Ç–∞|–º–µ–π–ª)',
                    r'(?:—ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–∞—è\s+)?–ø–æ—á—Ç–∞',
                ],
            }
        
        def classify_question(self, question: str) -> Tuple[Optional[str], float]:
            question_lower = question.lower().strip().rstrip('?')
            
            best_match = None
            best_confidence = 0.0
            
            for fact_type, patterns in self.question_patterns.items():
                for pattern in patterns:
                    match = re.search(pattern, question_lower)
                    if match:
                        matched_length = len(match.group())
                        total_length = len(question_lower)
                        confidence = matched_length / total_length if total_length > 0 else 0.5
                        
                        if matched_length == total_length:
                            confidence = 1.0
                        
                        if confidence > best_confidence:
                            best_match = fact_type
                            best_confidence = confidence
            
            return best_match, best_confidence
    
    class FactBasedRAGEngine:
        def __init__(self, fact_database: FactDatabase):
            self.fact_database = fact_database
            self.classifier = FactBasedQuestionClassifier()
        
        def process_question(self, question: str, dialogue_id: str) -> Tuple[str, Dict]:
            fact_type, confidence = self.classifier.classify_question(question)
            
            if not fact_type:
                return self._process_unknown_question(question, dialogue_id)
            
            relevant_facts = self.fact_database.query_facts(
                dialogue_id,
                fact_type=fact_type,
                min_confidence=0.5
            )
            
            if relevant_facts:
                return self._create_fact_based_prompt(question, relevant_facts, fact_type)
            else:
                return self._create_no_info_prompt(question, fact_type)
        
        def _create_fact_based_prompt(self, question: str, facts: List[Fact], 
                                     fact_type: str) -> Tuple[str, Dict]:
            high_confidence = [f for f in facts if f.confidence.score >= 0.8]
            medium_confidence = [f for f in facts if 0.5 <= f.confidence.score < 0.8]
            
            prompt_parts = []
            
            if high_confidence:
                prompt_parts.append("–¢–û–ß–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø:")
                for fact in high_confidence:
                    prompt_parts.append(f"‚Ä¢ {fact.to_natural_text()}")
            
            if medium_confidence:
                prompt_parts.append("\n–í–û–ó–ú–û–ñ–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø:")
                for fact in medium_confidence:
                    prompt_parts.append(f"‚Ä¢ {fact.to_natural_text()} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {fact.confidence.score:.0%})")
            
            prompt_parts.append(f"\n–í–æ–ø—Ä–æ—Å: {question}")
            prompt_parts.append("–û—Ç–≤–µ—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ñ–∞–∫—Ç–æ–≤. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —Ç–∞–∫ –∏ —Å–∫–∞–∂–∏.")
            
            metadata = {
                'strategy': 'fact_based',
                'fact_type': fact_type,
                'facts_found': len(facts),
                'confidence': 0.8  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è
            }
            
            return "\n".join(prompt_parts), metadata
        
        def _create_no_info_prompt(self, question: str, fact_type: str) -> Tuple[str, Dict]:
            no_info_responses = {
                FactType.WORK_OCCUPATION: "–í –¥–∏–∞–ª–æ–≥–µ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.",
                FactType.PERSONAL_NAME: "–í –¥–∏–∞–ª–æ–≥–µ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –∏–º–µ–Ω–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.",
                FactType.PERSONAL_AGE: "–í –¥–∏–∞–ª–æ–≥–µ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –≤–æ–∑—Ä–∞—Å—Ç–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.",
                FactType.SPORT_TYPE: "–í –¥–∏–∞–ª–æ–≥–µ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å–ø–æ—Ä—Ç–∏–≤–Ω—ã—Ö –∑–∞–Ω—è—Ç–∏—è—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.",
                FactType.PET_TYPE: "–í –¥–∏–∞–ª–æ–≥–µ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–∏—Ç–æ–º—Ü–∞—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.",
                FactType.TRANSPORT_CAR_BRAND: "–í –¥–∏–∞–ª–æ–≥–µ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –∞–≤—Ç–æ–º–æ–±–∏–ª–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.",
                FactType.DRINK_COFFEE: "–í –¥–∏–∞–ª–æ–≥–µ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è—Ö –≤ –∫–æ—Ñ–µ.",
                FactType.FINANCE_INCOME: "–í –¥–∏–∞–ª–æ–≥–µ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –¥–æ—Ö–æ–¥–∞—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.",
                FactType.EDUCATION_INSTITUTION: "–í –¥–∏–∞–ª–æ–≥–µ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.",
                FactType.HOBBY_ACTIVITY: "–í –¥–∏–∞–ª–æ–≥–µ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ö–æ–±–±–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.",
                FactType.HEALTH_CONDITION: "–í –¥–∏–∞–ª–æ–≥–µ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∑–¥–æ—Ä–æ–≤—å–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.",
                FactType.PROPERTY_TYPE: "–í –¥–∏–∞–ª–æ–≥–µ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∂–∏–ª—å–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.",
                FactType.CONTACT_PHONE: "–í –¥–∏–∞–ª–æ–≥–µ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–æ–Ω—Ç–∞–∫—Ç–∞—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.",
                FactType.CONTACT_EMAIL: "–í –¥–∏–∞–ª–æ–≥–µ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± email –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.",
            }
            
            response = no_info_responses.get(
                fact_type,
                f"–í –¥–∏–∞–ª–æ–≥–µ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å: {question}"
            )
            
            metadata = {
                'strategy': 'no_info',
                'fact_type': fact_type,
                'facts_found': 0
            }
            
            return response, metadata
        
        def _process_unknown_question(self, question: str, dialogue_id: str) -> Tuple[str, Dict]:
            all_facts = self.fact_database.query_facts(dialogue_id, query=question)
            
            if all_facts:
                prompt = f"–ù–∞–π–¥–µ–Ω–∞ —Å–ª–µ–¥—É—é—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:\n"
                for fact in all_facts[:5]:
                    prompt += f"‚Ä¢ {fact.to_natural_text()}\n"
                prompt += f"\n–í–æ–ø—Ä–æ—Å: {question}\n"
                prompt += "–û—Ç–≤–µ—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."
            else:
                prompt = f"–í –¥–∏–∞–ª–æ–≥–µ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å: {question}"
            
            metadata = {
                'strategy': 'keyword_search',
                'fact_type': None,
                'facts_found': len(all_facts)
            }
            
            return prompt, metadata


def load_dialogue(file_path: str) -> Dict[str, Any]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∏–∞–ª–æ–≥ –∏–∑ —Ñ–∞–π–ª–∞"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def is_copy_paste_content(content: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–æ–ø–∏–ø–∞—Å—Ç"""
    content_lower = content.lower()
    
    recommendation_markers = [
        '–ø–æ—Å–æ–≤–µ—Ç—É–π', '—Ä–∞—Å—Å–∫–∞–∂–∏', '—á—Ç–æ –º–æ–∂–Ω–æ', '–∫—É–¥–∞ –ø–æ–µ—Ö–∞—Ç—å',
        '—á–µ–º –∑–∞–Ω—è—Ç—å—Å—è', '–ø–æ–±–æ–ª—å—à–µ –æ', '—Å –∫–∞–∫–∏–º', '–≥–¥–µ –º–æ–∂–Ω–æ',
        '–∞ –æ–Ω –¥–æ—Ä–æ–≥–æ–π', '–ª—É—á—à–µ –≤–∑—è—Ç—å', '–Ω–µ —Ö–æ—á—É –Ω–∞ –∞–≤–∏—Ç–æ',
        '–∫–∞–∫–æ–π', '–∫–∞–∫–∞—è', '–∫–∞–∫–∏–µ', '–∫–∞–∫', '—á—Ç–æ', '–≥–¥–µ', '–∫–æ–≥–¥–∞',
        '–ø–æ—á–µ–º—É', '–∫—É–¥–∞', '—á–µ–º', '—Ä–∞—Å—Å–∫–∞–∂–∏', '–æ–±—ä—è—Å–Ω–∏',
        '–æ—Ç–∫—É–¥–∞', '–æ—Ç–∫—É–¥–∞ —ç—Ç–æ—Ç', '—Ä–∞—Å—Å–∫–∞–∂–∏ –ø—Ä–∏–Ω—Ü–∏–ø', '–≤–æ –≤—Å–µ—Ö –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç—è—Ö',
        '–º–Ω–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ', '—Ö–æ—á—É —É–∑–Ω–∞—Ç—å', '—Ä–∞—Å—Å–∫–∞–∂–∏ –º–Ω–µ',
        '—á—Ç–æ –¥–µ–ª–∞—Ç—å', '–∫–∞–∫ –±—ã—Ç—å', '—á—Ç–æ –ø–æ—Å–æ–≤–µ—Ç—É–µ—à—å'
    ]
    
    for marker in recommendation_markers:
        if marker in content_lower:
            return True
    
    if len(content) > 200:
        return True
        
    question_words = ['?', '—á—Ç–æ ', '–≥–¥–µ ', '–∫–∞–∫ ', '–∫–æ–≥–¥–∞ ', '–ø–æ—á–µ–º—É ', 
                     '–∫—É–¥–∞ ', '—á–µ–º ', '–∫–∞–∫–æ–π ', '–∫–∞–∫–∞—è ', '–∫–∞–∫–∏–µ ']
    is_question = any(q in content_lower for q in question_words)
    
    if is_question:
        return True
        
    return False


def contains_personal_info(content: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –ª–∏—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é"""
    content_lower = content.lower()
    
    personal_indicators = [
        '—è', '–º–µ–Ω—è', '–º–æ–π', '–º–æ—è', '–º–Ω–µ', '—É –º–µ–Ω—è',
        '–º—ã', '–Ω–∞—Å', '–Ω–∞—à', '–Ω–∞—à–∞', '–Ω–∞–º', '—É –Ω–∞—Å',
        '—Å–µ–º—å—è', '—Å–µ–º—å–µ', '–¥–µ—Ç–µ–π', '–∂–µ–Ω–∞', '–º—É–∂',
        '—Å—ã–Ω', '–¥–æ—á—å', '—Ä–µ–±–µ–Ω–æ–∫', '–¥–µ—Ç–∏',
        '—Ä–∞–±–æ—Ç–∞—é', '–∂–∏–≤—É', '–µ–∑–∂—É', '–∏–º–µ—é', '–≤–ª–∞–¥–µ—é'
    ]
    
    return any(ind in content_lower for ind in personal_indicators)


def extract_user_messages_only(messages: List[Dict[str, Any]]) -> List[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¢–û–õ–¨–ö–û —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""
    user_messages = []
    
    for msg in messages:
        role = msg.get("role", "")
        content = msg.get("content", "").strip()
        
        if role != "user":
            continue
            
        if not content:
            continue
            
        if is_copy_paste_content(content):
            continue
            
        if len(content) < 10 or len(content) > 300:
            continue
            
        if contains_personal_info(content):
            user_messages.append(content)
    
    return user_messages


def extract_facts_from_messages(user_messages: List[str], dialogue_id: str, 
                               session_id: str) -> List[Fact]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ñ–∞–∫—Ç—ã –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    facts = []
    
    for message in user_messages:
        message_lower = message.lower()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∞–∫—Ç—ã –æ —Ä–∞–±–æ—Ç–µ
        if '—Ä–∞–±–æ—Ç–∞—é' in message_lower:
            if '–ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç' in message_lower:
                facts.append(Fact(
                    type=FactType.WORK_OCCUPATION,
                    subject="–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å",
                    relation=FactRelation.WORKS_AS,
                    object="–ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç",
                    confidence=FactConfidence(0.9),
                    session_id=session_id,
                    dialogue_id=dialogue_id
                ))
            elif '—É—á–∏—Ç–µ–ª—å' in message_lower:
                facts.append(Fact(
                    type=FactType.WORK_OCCUPATION,
                    subject="–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å",
                    relation=FactRelation.WORKS_AS,
                    object="—É—á–∏—Ç–µ–ª—å",
                    confidence=FactConfidence(0.9),
                    session_id=session_id,
                    dialogue_id=dialogue_id
                ))
            elif '–≤—Ä–∞—á' in message_lower:
                facts.append(Fact(
                    type=FactType.WORK_OCCUPATION,
                    subject="–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å",
                    relation=FactRelation.WORKS_AS,
                    object="–≤—Ä–∞—á",
                    confidence=FactConfidence(0.9),
                    session_id=session_id,
                    dialogue_id=dialogue_id
                ))
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∞–∫—Ç—ã –æ —Å–ø–æ—Ä—Ç–µ
        if '—Å–ø–æ—Ä—Ç' in message_lower:
            if '–∫–æ—Å—Ç—é–º' in message_lower and '–æ—Ç–∫–∞–∑—ã–≤–∞—é—Å—å' in message_lower:
                facts.append(Fact(
                    type=FactType.SPORT_TYPE,
                    subject="–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å",
                    relation=FactRelation.HAS,
                    object="–Ω–µ –Ω–æ—Å–∏—Ç —Å–ø–æ—Ä—Ç–∏–≤–Ω—ã–µ –∫–æ—Å—Ç—é–º—ã",
                    confidence=FactConfidence(0.8),
                    session_id=session_id,
                    dialogue_id=dialogue_id
                ))
            elif '—Ñ—É—Ç–±–æ–ª' in message_lower:
                facts.append(Fact(
                    type=FactType.SPORT_TYPE,
                    subject="–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å",
                    relation=FactRelation.LIKES,
                    object="—Ñ—É—Ç–±–æ–ª",
                    confidence=FactConfidence(0.9),
                    session_id=session_id,
                    dialogue_id=dialogue_id
                ))
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∞–∫—Ç—ã –æ –ø–∏—Ç–æ–º—Ü–∞—Ö
        if '—Å–æ–±–∞–∫–∞' in message_lower or '–ø–µ—Å' in message_lower:
            if '–ª–∞–±—Ä–∞–¥–æ—Ä' in message_lower:
                facts.append(Fact(
                    type=FactType.PET_TYPE,
                    subject="–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å",
                    relation=FactRelation.OWNS,
                    object="—Å–æ–±–∞–∫–∞ –ø–æ—Ä–æ–¥—ã –ª–∞–±—Ä–∞–¥–æ—Ä",
                    confidence=FactConfidence(0.9),
                    session_id=session_id,
                    dialogue_id=dialogue_id
                ))
            elif '–æ–≤—á–∞—Ä–∫–∞' in message_lower:
                facts.append(Fact(
                    type=FactType.PET_TYPE,
                    subject="–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å",
                    relation=FactRelation.OWNS,
                    object="—Å–æ–±–∞–∫–∞ –ø–æ—Ä–æ–¥—ã –æ–≤—á–∞—Ä–∫–∞",
                    confidence=FactConfidence(0.9),
                    session_id=session_id,
                    dialogue_id=dialogue_id
                ))
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∞–∫—Ç—ã –æ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–µ
        if '–º–∞—à–∏–Ω–∞' in message_lower or '–∞–≤—Ç–æ' in message_lower:
            if '—Ç–æ–π–æ—Ç–∞' in message_lower:
                facts.append(Fact(
                    type=FactType.TRANSPORT_CAR_BRAND,
                    subject="–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å",
                    relation=FactRelation.OWNS,
                    object="Toyota",
                    confidence=FactConfidence(0.9),
                    session_id=session_id,
                    dialogue_id=dialogue_id
                ))
            elif '–º–µ—Ä—Å–µ–¥–µ—Å' in message_lower:
                facts.append(Fact(
                    type=FactType.TRANSPORT_CAR_BRAND,
                    subject="–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å",
                    relation=FactRelation.OWNS,
                    object="Mercedes",
                    confidence=FactConfidence(0.9),
                    session_id=session_id,
                    dialogue_id=dialogue_id
                ))
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∞–∫—Ç—ã –æ –≤–æ–∑—Ä–∞—Å—Ç–µ
        age_match = re.search(r'(\d+)\s*–ª–µ—Ç', message_lower)
        if age_match:
            facts.append(Fact(
                type=FactType.PERSONAL_AGE,
                subject="–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å",
                relation=FactRelation.IS,
                object=f"{age_match.group(1)} –ª–µ—Ç",
                confidence=FactConfidence(0.9),
                session_id=session_id,
                dialogue_id=dialogue_id
            ))
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∞–∫—Ç—ã –æ–± –∏–º–µ–Ω–∏
        name_match = re.search(r'(?:–∑–æ–≤—É—Ç|–∏–º—è)\s+([–∞-—è—ë]+)', message_lower)
        if name_match:
            facts.append(Fact(
                type=FactType.PERSONAL_NAME,
                subject="–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å",
                relation=FactRelation.IS,
                object=name_match.group(1),
                confidence=FactConfidence(0.9),
                session_id=session_id,
                dialogue_id=dialogue_id
            ))
    
    return facts


def create_enhanced_prompt_from_dialogue(dialogue: Dict[str, Any], 
                                       fact_database: FactDatabase) -> str:
    """–°–æ–∑–¥–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏–∞–ª–æ–≥–∞ –∏ —Ñ–∞–∫—Ç–æ–≤"""
    dialogue_id = dialogue.get("id", "unknown")
    question = dialogue.get("question", "–ö–∞–∫ –º–µ–Ω—è –∑–æ–≤—É—Ç?")
    
    # –°–æ–±–∏—Ä–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    all_user_messages = []
    for session in dialogue.get("sessions", []):
        session_messages = session.get("messages", [])
        user_messages = extract_user_messages_only(session_messages)
        all_user_messages.extend(user_messages)
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∞–∫—Ç—ã
    all_facts = []
    for session in dialogue.get("sessions", []):
        session_id = session.get("id", "unknown")
        session_messages = session.get("messages", [])
        user_messages = extract_user_messages_only(session_messages)
        facts = extract_facts_from_messages(user_messages, dialogue_id, session_id)
        all_facts.extend(facts)
        fact_database.facts.extend(facts)
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º RAG –¥–≤–∏–∂–æ–∫
    rag_engine = FactBasedRAGEngine(fact_database)
    prompt, metadata = rag_engine.process_question(question, dialogue_id)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    context_parts = [f"–î–∏–∞–ª–æ–≥ ID: {dialogue_id}", f"–í–æ–ø—Ä–æ—Å: {question}", ""]
    
    if all_user_messages:
        context_parts.append("–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ:")
        for i, msg in enumerate(all_user_messages[-10:], 1):
            context_parts.append(f"{i}. {msg}")
    else:
        context_parts.append("–õ–∏—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –¥–∏–∞–ª–æ–≥–µ.")
    
    context_parts.extend(["", prompt])
    
    return "\n".join(context_parts), metadata


def process_dialogue_enhanced(dialogue: Dict[str, Any], 
                            fact_database: FactDatabase) -> Dict[str, Any]:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–∏–∞–ª–æ–≥ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ñ–∞–∫—Ç-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞"""
    try:
        dialogue_id = dialogue.get("id", "unknown")
        question = dialogue.get("question", "–ö–∞–∫ –º–µ–Ω—è –∑–æ–≤—É—Ç?")
        
        # –°–æ–∑–¥–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        prompt, metadata = create_enhanced_prompt_from_dialogue(dialogue, fact_database)
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        total_messages = 0
        user_messages_count = 0
        filtered_messages_count = 0
        
        for session in dialogue.get("sessions", []):
            session_messages = session.get("messages", [])
            total_messages += len(session_messages)
            
            for msg in session_messages:
                if msg.get("role") == "user":
                    user_messages_count += 1
                    content = msg.get("content", "").strip()
                    
                    if (len(content) >= 10 and len(content) <= 300 and 
                        not is_copy_paste_content(content) and 
                        contains_personal_info(content)):
                        filtered_messages_count += 1
        
        return {
            "dialogue_id": dialogue_id,
            "question": question,
            "prompt": prompt,
            "metadata": metadata,
            "total_messages": total_messages,
            "user_messages": user_messages_count,
            "filtered_messages": filtered_messages_count,
            "sessions_count": len(dialogue.get("sessions", [])),
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–∏–∞–ª–æ–≥–∞: {e}")
        import traceback
        traceback.print_exc()
        return {
            "dialogue_id": dialogue.get("id", "unknown"),
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }


def run_enhanced_inference(dataset_path: str, output_path: str):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–π –∏–Ω—Ñ–µ—Ä–µ–Ω—Å —Å —Ñ–∞–∫—Ç-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –ø–æ–¥—Ö–æ–¥–æ–º"""
    print(f"üöÄ –ó–∞–ø—É—Å–∫ –£–õ–£–ß–®–ï–ù–ù–û–ì–û –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ —Å —Ñ–∞–∫—Ç-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –ø–æ–¥—Ö–æ–¥–æ–º: {dataset_path}")
    
    # –°–æ–∑–¥–∞–µ–º –≤—ã—Ö–æ–¥–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    output_dir = Path(output_path)
    output_dir.mkdir(exist_ok=True, parents=True)
    
    # –°–æ–∑–¥–∞–µ–º –±–∞–∑—É —Ñ–∞–∫—Ç–æ–≤
    fact_database = FactDatabase()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
    dialogues = []
    with open(dataset_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                dialogues.append(json.loads(line))
    
    print(f"üìñ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(dialogues)} –¥–∏–∞–ª–æ–≥–æ–≤")
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –¥–∏–∞–ª–æ–≥
    results = []
    prompts = []
    
    for i, dialogue in enumerate(dialogues):
        print(f"‚öôÔ∏è –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∏–∞–ª–æ–≥ {i+1}/{len(dialogues)}: {dialogue.get('id', 'unknown')}")
        result = process_dialogue_enhanced(dialogue, fact_database)
        results.append(result)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–ø—Ç –æ—Ç–¥–µ–ª—å–Ω–æ
        if "prompt" in result:
            prompts.append({
                "dialogue_id": result["dialogue_id"],
                "prompt": result["prompt"],
                "metadata": result.get("metadata", {})
            })
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    output_file = output_dir / "results.jsonl"
    with open(output_file, 'w', encoding='utf-8') as f:
        for result in results:
            f.write(json.dumps(result, ensure_ascii=False) + '\n')
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–ø—Ç—ã –æ—Ç–¥–µ–ª—å–Ω–æ
    prompts_file = output_dir / "prompts.jsonl"
    with open(prompts_file, 'w', encoding='utf-8') as f:
        for prompt_data in prompts:
            f.write(json.dumps(prompt_data, ensure_ascii=False) + '\n')
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–ø—Ç—ã –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã
    prompts_dir = output_dir / "prompt_files"
    prompts_dir.mkdir(exist_ok=True)
    
    for i, prompt_data in enumerate(prompts):
        prompt_file = prompts_dir / f"prompt_{prompt_data['dialogue_id']}.txt"
        with open(prompt_file, 'w', encoding='utf-8') as f:
            f.write(prompt_data["prompt"])
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –±–∞–∑—É —Ñ–∞–∫—Ç–æ–≤
    facts_file = output_dir / "facts.jsonl"
    with open(facts_file, 'w', encoding='utf-8') as f:
        for fact in fact_database.facts:
            fact_data = {
                "type": fact.type,
                "subject": fact.subject,
                "relation": fact.relation,
                "object": fact.object,
                "confidence": fact.confidence.score,
                "session_id": fact.session_id,
                "dialogue_id": fact.dialogue_id
            }
            f.write(json.dumps(fact_data, ensure_ascii=False) + '\n')
    
    print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}")
    print(f"üíæ –ü—Ä–æ–º–ø—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {prompts_file}")
    print(f"üíæ –û—Ç–¥–µ–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã –ø—Ä–æ–º–ø—Ç–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {prompts_dir}")
    print(f"üíæ –ë–∞–∑–∞ —Ñ–∞–∫—Ç–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {facts_file}")
    print(f"üìä –í—Å–µ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–æ —Ñ–∞–∫—Ç–æ–≤: {len(fact_database.facts)}")
    print("‚úÖ –£–õ–£–ß–®–ï–ù–ù–´–ô –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
    
    return 0


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(description="Enhanced Fact-Based Dialogue Inference")
    parser.add_argument("--dataset", type=str, required=True, help="–ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞")
    parser.add_argument("--output", type=str, default="./enhanced_output", help="–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    
    args = parser.parse_args()
    
    return run_enhanced_inference(args.dataset, args.output)


if __name__ == "__main__":
    sys.exit(main())
